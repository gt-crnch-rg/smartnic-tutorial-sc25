# Table of Contents
1. [Setup](#setup)
2. [Compiling](#compiling)
3. [Running](#running)
4. [Results](#results)

# Setup

## Accessing Nodes

Log in the system using credentials in the [sheet](https://docs.google.com/spreadsheets/d/1-tSZ6L0IcdD1Gn2FcMJeT2euWquRxyOD1YoBo517N_M/edit?usp=sharing).

Assign yourself a node by adding your name to the sheet.

Login to the proxy node
```
ssh rdmaworkshopXX@155.248.177.18
```
replace XX with number of the user assigned to you in sheet.

Login to the cluster node from there
```
ssh gw.hpcadvisorycouncil.com
```


## Package
Copy your package folder
```
cp /var/tmp/mpi-odos-pkg/ ~/
```
Move into the tutorial directorey
```
cd ~/mpi-odos-pkg/tutorial/
```

# Compiling
to compile, run
```
. compile.sh
```
This script will compile following and move the results to `out/` folder
- Tasks a to c
- OSU Microbenchmarks
- miniWeather Application

# Running
Run using the scripts and observe the files generated by `sbatch` to observe and analyze results. Make sure about two important details to get a correct run:
- Wait a few seconds to launch task after launching Service
- ⚠️  **Make sure that the service completes before next run. If it does not, use `scancel` to stop it.**

## Running Tasks
The tasks can be run using scripts `run_service.sh` (no arguments) and `run_task.sh` (binary as an argument).
```
sbatch run_service.sh
sbatch run_task.sh out/tasks/a_hello
```
Further task binaries are following:

- `a_hello`
- `b_omp_parallel`
- `c_shared`

in folder `out/tasks/`

Please look into tasks folders for further details about running these tasks manually.

## Running OSU Microbenchmarks

### Point-to-Point
```
sbatch run_service_mpi_pt2pt.sh
sbatch run_task_pt2pt.sh out/omb/pt2pt/bw_h2d
```
Further point-to-point communication binaries are present in `out/omb/pt2pt/`. All these binaries are enlisted in the following table.

| Metric Name               | host-to-device   | device-to-host   | device-to-device | host-to-host     |
|---------------------------|------------------|------------------|------------------|------------------|
| Bidirectional Bandwidth   | bibw_h2d         | bibw_d2h         | bibw_d2d         | bibw_h2h         |
| Bandwidth                 | bw_h2d           | bw_d2h           | bw_d2d           | bw_h2h           |
| Latency                   | latency_h2d      | latency_d2h      | latency_d2d      | latency_h2h      |
| Multi-threaded Latency    | latency_mt_h2d   | latency_mt_d2h   | latency_mt_d2d   | latency_mt_h2h   |
| Max Bandwidth (MR)        | mbw_mr_h2d       | mbw_mr_d2h       | mbw_mr_d2d       | mbw_mr_h2h       |
| Multi-Process Latency     | multi_lat_h2d    | multi_lat_d2h    | multi_lat_d2d    | multi_lat_h2h    |

### Remote Memory Access / One-Sided Communication
```
sbatch run_service_mpi_1sc.sh
sbatch run_task_1sc.sh out/omb/one-sided/get_bw_h2d
```
Further one-sided communication binaries are present in `out/omb/one-sided/`. All these binaries are enlisted in the following table.

| Metric Name               | host-to-device   | device-to-host   | device-to-device | host-to-host     |
|----------------------------|------------------------|------------------------|------------------------|------------------------|
| Accumulate Latency         | acc_latency_h2d        | acc_latency_d2h        | acc_latency_d2d        | acc_latency_h2h        |
| Compare-And-Swap Latency   | cas_latency_h2d        | cas_latency_d2h        | cas_latency_d2d        | cas_latency_h2h        |
| Fetch-and-Op Latency       | fop_latency_h2d        | fop_latency_d2h        | fop_latency_d2d        | fop_latency_h2h        |
| Get Accumulate Latency     | get_acc_latency_h2d    | get_acc_latency_d2h    | get_acc_latency_d2d    | get_acc_latency_h2h    |
| Get Bandwidth              | get_bw_h2d             | get_bw_d2h             | get_bw_d2d             | get_bw_h2h             |
| Get Latency                | get_latency_h2d        | get_latency_d2h        | get_latency_d2d        | get_latency_h2h        |
| Put Bidirectional Bandwidth| put_bibw_h2d           | put_bibw_d2h           | put_bibw_d2d           | put_bibw_h2h           |
| Put Bandwidth              | put_bw_h2d             | put_bw_d2h             | put_bw_d2d             | put_bw_h2h             |
| Put Latency                | put_latency_h2d        | put_latency_d2h        | put_latency_d2d        | put_latency_h2h        |


### Collective
```
sbatch run_service_mpi.sh
sbatch run_task_mpi.sh out/omb/collective/ialltoall
```

Further one-sided communication binaries are present in `out/omb/collective/`. All these binaries are enlisted in the following table.


| Collective Operation      | Blocking         | Non-Blocking       |
|---------------------------|------------------|--------------------|
| Allgather                 | allgather        | iallgather         |
| Allgatherv                | allgatherv       | -                  |
| Allreduce                 | allreduce        | -                  |
| Alltoall                  | alltoall         | ialltoall          |
| Alltoallv                 | alltoallv        | ialltoallv         |
| Alltoallw                 | -                | ialltoallw         |
| Barrier                   | -                | ibarrier           |
| Broadcast                 | -                | ibcast             |
| Gather                    | gather           | igather            |
| Gatherv                   | -                | igatherv           |
| Scatter                   | scatter          | iscatter           |
| Scatterv                  | scatterv         | iscatterv          |
| Reduce                    | reduce           | -                  |
| Reduce-Scatter            | reduce_scatter   | -                  |


## Running miniWeather

### Refernce Run
```
sbatch run_service_mpi.sh
sbatch run_task_mpi.sh out/miniweather/miniWeather_mpi_omp
```
### ODOS Accelerated Run
```
sbatch run_service_mpi.sh
sbatch run_task_mpi.sh out/miniweather/miniWeather_mpi_odos_omp 0.325
```
The number `0.325` in command represents the ratio of region whose compute is offloaded to the DPU. 32.5% in DPU and 67.5% in host. We can change this number to offload more or less region to SmartNIC.

# Results

## Task Run
This section documents the output for the above scripts.
### Task Output
```
cmd : out/tasks/a_hello
```
### Service Output
```
Hi Folks!
```
## OSU Mircobenchmark Run
### Point-to-Point Communication
#### Task Output
```
cmd : out/omb/pt2pt/bw_h2d
going to connect....
connected [OMPI_PML]
going to connect....
connected [OMPI_PML]
# OSU MPI ODOS-H2D Bandwidth Test
# Size        Bandwidth (MB/s)
1                         2.09
2                         4.41
4                         8.64
8                        16.95
16                       35.03
32                       69.85
64                      132.98
128                     275.50
256                     519.90
512                     995.54
1024                   1992.96
2048                   4075.89
4096                   5828.46
8192                   8802.57
16384                 17287.07
32768                 19598.01
65536                 20720.76
131072                21563.68
262144                21305.41
524288                22132.76
1048576               22106.51
2097152               22267.81
4194304               22559.20
```
#### Service Output
```
dpu is listening... [OMPI_PML]
dpu is listening... [OMPI_PML]
[server] server name: omp_test
[server] server name: omp_test
```

### Remote Memory Access / One-Sided Communication
#### Task Output
```
cmd : out/omb/one-sided/get_bw_h2d
# OSU MPI_Get Bandwidth Test
# Window creation: MPI_Win_create
# Synchronization: MPI_Win_lock/unlock
# Size        Bandwidth (MB/s)
going to connect....
connected [OMPI_OSC]
going to connect....
connected [OMPI_OSC]
1                         1.62
2                         3.25
4                         6.52
8                        12.85
16                       26.00
32                       51.91
64                      102.05
128                     190.62
256                     376.15
512                     809.28
1024                   1947.21
2048                   3852.83
4096                   7467.16
8192                  12606.91
16384                 16783.95
32768                 20132.50
65536                 22220.69
131072                23538.41
262144                24095.57
524288                24404.54
1048576               24532.79
2097152               24600.13
4194304               24643.03
```
#### Service Output
```
[server] server name: omp_test
[server] server name: omp_test
dpu is listening... [OMPI_OSC]
dpu is listening... [OMPI_OSC]
```
### Collectives
#### Task Output
```
cmd : out/omb/collective/ialltoall

# OSU MPI Non-blocking All-to-All Latency Test
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size             Overall(us)         Compute(us)      Pure Comm.(us)          Overlap(%)
1                        66.88               64.32               61.70               95.85
2                        48.40               45.69               43.38               93.75
4                        48.34               45.69               43.34               93.89
8                        48.90               45.61               43.25               92.40
16                       48.24               45.61               43.20               93.91
32                       48.79               46.19               43.56               94.04
64                       48.92               46.22               43.71               93.82
128                      48.89               46.17               43.65               93.78
256                      49.24               46.45               44.58               93.73
512                      49.92               47.12               44.94               93.77
1024                     50.60               47.69               45.47               93.60
2048                     51.78               48.80               46.08               93.55
4096                     51.77               49.02               47.48               94.21
8192                     57.04               54.33               51.76               94.77
16384                    59.81               56.93               54.59               94.72
32768                    65.07               61.70               58.93               94.28
65536                    75.57               72.10               69.67               95.02
131072                   95.74               92.70               90.18               96.62
262144                  127.14              123.68              120.62               97.13
524288                  186.42              182.57              178.33               97.84
1048576                 302.82              298.75              292.44               98.61
2097152                 549.93              540.72              530.57               98.26
4194304                1060.45             1048.20             1029.94               98.81
```
#### Service Output
```
[server] server name: omp_test
[server] server name: omp_test
[server] server name: omp_test
[server] server name: omp_test
```

## miniWeather
### Refernce
#### Task Output
```
cmd : out/miniweather/miniWeather_mpi_omp
----------------------------------------
nx_glob, nz_glob:  4096  2048
nx,nz           :  1024  2048
dt              : 0.01628
----------------------------------------
>> time: 32.747 seconds
----------------------------------------
Profile
-------
  halo x mpi  :      0.201 seconds
  halo x omp  :      0.000 seconds
  tend x      :      9.286 seconds
    launch        :      0.000 seconds
    host          :      9.285 seconds
    wait          :      0.000 seconds
  halo tend z :      9.306 seconds
    launch        :      0.000 seconds
    host          :      9.306 seconds
    wait          :      0.000 seconds
  compute     :     13.955 seconds
    launch        :      0.000 seconds
    host          :     13.954 seconds
    wait          :      0.000 seconds
----------------------------------------
```
#### Service Output
```
[server] server name: omp_test
[server] server name: omp_test
[server] server name: omp_test
[server] server name: omp_test
```
### ODOS Accelerated
#### Task Output
```
cmd : out/miniweather/miniWeather_mpi_odos_omp 0.325
----------------------------------------
nx_glob, nz_glob:  4096  2048
nx,nz           :  1024  2048
nxh,nxd         :   691   332 [166,166]
dx,dz           : 4.883 4.883
dt              : 0.01628
----------------------------------------
ratio      :  0.32500
state      :  1028 [ 1024+2*2] x  2052 [ 2048+2*2] x 4
state host :   695 [  691+2*2] x  2052 [ 2048+2*2] x 4
state  dpu :   170 [  166+2*2] x  2052 [ 2048+2*2] x 4 x 2
----------------------------------------
>> time: 24.068 seconds
----------------------------------------
Profile
-------
  halo x mpi  :      0.528 seconds
  halo x omp  :      0.367 seconds
  tend x      :      6.653 seconds
    launch        :      0.001 seconds
    host          :      6.476 seconds
    wait          :      0.175 seconds
  halo tend z :      7.140 seconds
    launch        :      0.019 seconds
    host          :      6.701 seconds
    wait          :      0.420 seconds
  compute     :      9.379 seconds
    launch        :      0.042 seconds
    host          :      9.333 seconds
    wait          :      0.004 seconds
----------------------------------------
```
#### Service Output
```
[server] server name: omp_test
[server] server name: omp_test
[server] server name: omp_test
[server] server name: omp_test
```
